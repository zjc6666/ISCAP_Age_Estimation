# rnnlm related
layer: 2
unit: 650 # 1024
opt: sgd  # sgd        # or adam
batchsize: 64 # 64   # batch size in LM training
epoch: 50     # if the data size is large, we can reduce this
patience: 0
maxlen: 100 # 100     # if sentence length > lm_maxlen, lm_batchsize is automatically reduced
